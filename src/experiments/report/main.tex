% !TeX root = main.tex
\input{preamble}

\title{Project Final Report: Modeling and Generating Realistic Workloads}
\author{JianChen Zhao}
\email{jianchen.zhao@uwaterloo.ca}
\addbibresource{paperpile.bib}

\begin{document}

\begin{abstract}
% todo
\end{abstract}

\maketitle

\section{Introduction}

In today's dynamic and interconnected digital landscape, where user expectations for speed and reliability are higher than ever, workload testing is indispensable. For instance, Ticketmaster, a well-known ticket vendor, faced technical difficulties such as server crashes and increased latency during a pre-sale event for Taylor Swift's 2022 tour. Despite preregistrations for the event, their system faltered due to inadequate load testing \cite{Reuters2022-dc}. In contrast, during Prime Day 2023, Amazon handled an astounding number of requests, including 764 petabytes of EBS data transfer, over 15.35 trillion requests, 830 CloudTrail events, and 500 million CloudFront HTTP requests per minute. Amazon would not have achieved such a feat without rigorous load testing\cite{Barr2023-wr}.

As software systems become more complex and are deployed on increasingly diverse platforms, assessing their robustness under varying workloads becomes paramount. Large-scale software systems like cloud services are deeply embedded into today's economy. Many businesses's critical operations rely on such software. To illustrate, the global end-user spending in public cloud services is more than half a trillion USD in 2023\cite{UnknownUnknown-aj}. For operators of such large-scale software systems, a failure can result in significant financial loss and impact business worldwide\cite{LuuUnknown-mo}; Facebook, for example, lost an estimated 90 million USD to a 14-hour outage caused by misconfigured DNS\cite{AtlassianUnknown-sw}. Interestingly, such failures can not be attributed to functional bugs\cite{Weyuker2000-gw}. It is, therefore, crucial that systems are rigorously tested as a whole.

\subsection{Background}

Among the various system testing paradigms, load testing emerges as a critical aspect, focusing on the system's ability to handle different levels of expected user activity. This form of testing empirically guarantees that a software application can meet its performance objectives under realistic working conditions.

Load testing involves simulating different scenarios and conditions that mimic the actual usage patterns and demands the software may encounter in a production environment. Subjecting the system to \emph{workloads}, including typical and peak loads, reduces system failure risks while helping identify potential bottlenecks, vulnerabilities, or performance degradation that may not arise in functional tests \cite{Syer2017-ek, Shang2015-gj, Cohen2005-mn, Hassan2008-nj, Chen2016-bo}.

The most basic form of load testing consists of replaying the system against a manually collected dataset of user actions. This approach requires extensive tooling of the deployed system to capture the user actions and ample storage to accommodate the large number of workloads necessary to capture the variance in user actions. Many prior works have explored the automatically recovering workloads to work around these issues.

However, automatically generating workloads is no trivial task. There are two main challenges. First, the generated workload must be realistic. The action patterns in the workload must be representative of actual field observations. This property is essential to the usefulness of load testing. Testing on unrealistic data cannot ensure that the system under test can handle actual user actions. Second, real user actions have high variance; that is, there is a diverse number of action patterns in production environments. Hence, automatic workload generation techniques must balance the variance and representativeness of the recovered workload.

\subsection{Related Work}

Prior works on workload generation have explored automatically recovering workloads from various readily available sources, which are resampled for replay.

Many proposed approaches abstract away user behaviors in workload and only consider specific metrics. This reduces the information needed to be stored for replay. \cite{Shang2015-gj, Cohen2005-mn} use CPU usage metrics for high-level representation of user behaviors. Similarly, \cite{Haghdoost2017-bc, Yadwadkar2010-ml, Busch2015-yo, Seo2014-xv} characterize the load by the I/O usage, while \cite{Cortez2017-nc} looks at other system resource usage to model the workloads. While such coarse-grained approaches are efficient and easy to maintain, information about essential characteristics of the load might be lost.

As demonstrated in \cite{Cohen2005-mn}, more precise characterization of workloads may lead to better performance. \cite{Yadwadkar2010-ml} explored using a hidden Markov model to find patterns of user behaviors and found that only a small amount of data is required to construct an adequate model of the user behaviors.

To balance the granularity of the recovered workloads, \cite{Syer2017-ek, Vogele2018-zz, Summers2016-jj, Xi2011-ki, Hassan2008-nj} distinguishes different user behaviors by clustering user actions extracted from event logs or system traces. \cite{Chen2019-fu} adds to this approach by also considering sequences of user actions and their context, helping find more representative user behaviors without sacrificing the variance of the recovered workloads and introducing some adaptability to the approach. While this approach can achieve a good balance, it suffers from two problems. First, the approach clusters users based on their behaviors to sample actions and action sequences of a recovered workload using representative users. In effect, the diversity of the generated workload is restricted by the size of the source used to recover the initial workload. Second, event logs and traces are noisy. Actions are thus recovered by filtering on some user identifiers. However, those identifiers are not always present, and the recovered actions may be incomplete \cite{Zhao2023-nh}.

\subsection{Contributions}

To tackle the challenges above, we propose an extensible model of user behaviors based on a mixture of Hidden Markov Models (HMM) to allow the dynamic generation of workloads unconstrained by the size of the initial recovered workload. We evaluate our model and demonstrate its performance and generalizability. We also propose directions for future improvements.

\subsection{Organization}

We present our model in detail in \cref{sec:modeling_processes} while highlighting challenges in training such a model. We show in \cref{sec:approach} how it can be integrated to generate workloads dynamically. We evaluate it in \cref{sec:experimental_setup,sec:results}, and discuss tradeoffs and potential improvements in \cref{sec:discussion}. Finally, we conclude in \cref{sec:conclusion}.

\section{Modeling Event Logs}\label{sec:modeling_processes}

Event logs and system traces are discrete-time processes. Given time steps \(1, 2, ..., t, ..., \tau - 1, \tau\),  events \(y_t\) are observed according to an underlying process, that is, the control flow of a running software system.

\subsection{Control Flow Graph}

The control flow graph (CFG) of a program can be understood as the graphical representation of its state transitions at runtime. Nodes in this graph represent sequences of statements executed sequentially without branching except at the entry and exit statements. Edges represent the control flow and its branching between the basic blocks.

The CFG can also be seen as a description of the underlying process that produces outputs and, for our purposes, event logs and system traces. However, to get a complete picture of the production process, we need to compute the CFG for the whole program, or in other words, the interprocedural CFG (ICFG). Computing this ICFG is resource intensive and impractical and often must be approximated\cite{Zhao2023-nh}.

\subsection{Hidden Markov Chains (HMM)}

\begin{figure}
    \caption{An example of a graph of the states of a process, with associated state transition probabilities. This graph is analogous to a control flow graph.}
    \label{fig:state_graph}
    \vspace{2ex}
    \begin{tikzpicture}[x=1pt,y=1pt]
        \node [box,circle] (a) at (135:60) {$a$};
        \node [box,circle] (b) at (015:60) {$b$};
        \node [box,circle] (c) at (255:60) {$c$};
        \draw [->,thick] (a) -- (b) node [midway,fill=white] {$p_\mu(b|a)$};
        \draw [->,thick] (a) -- (c) node [midway,fill=white] {$p_\mu(c|a)$};
        \draw [->,thick] (b) -- (c) node [midway,fill=white] {$p_\mu(c|b)$};
    \end{tikzpicture}
\end{figure}

Instead of directly approximating an ICFG, we can fit a hidden Markov model (HMM) on a program's output to learn its states and transitions at runtime. The learned graph, where nodes are states and edges are transitions, is an indirect approximation of the ICFG with respect to the output. That is, it models the process that generates the outputs.

A HMM \(\mu = (X, Y)\) consists of two stochastic processes: \(X = \{X_t : 0 \leq t \leq \tau\}\) is a Markov process that is not directly observable, hence hidden, and \(Y = \{Y_t : 0 \leq t \leq \tau\}\) that is directly observed. Each \(X_t\) only depends on the previous \(X_{t-1}\) and each \(Y_t\) only depends on \(X_t\). In the remainder of this paper, we shall use the following shorthand for \(\mu = (X, Y)\): 
\[p_\mu(x) = P(X = x) \text{ and } p_\mu(x | y) = P(X = x | Y = y).\]
For simplicity, we assume that \(X\) is ergodic; that is, the probability of getting from every state to every other state is non-zero. This assumption should hold in practice; for load testing, we are concerned with system states that are reoccurring and don't lead to the termination of the system.

At a given discrete time \(t\), suppose the hidden process is at a state \(x_t\). The probability of t

A HMM intuitively models event logs and system traces. The process \(Y\) is the observed events, while the process \(X\) approximates the interprocedural control flow. The state transitions between 

The production of event logs is a stochastic process: each event is an element of a sequence indexed by time. Every time a software produces an event, it must go through a state transition following its control flow. However, Such a process can be modeled with an interleaved hidden Markov model (HMM). It combines a mixture of individual HMM, each modeling a single independent process. In load testing, tasks can be modeled by such a process. We can generate new workloads based on the processes using an interleaved HMM trained on those processes, leading to realistic yet high-variance synthetic workloads.


\subsection{Interleaved HMM}

\begin{figure}
    \caption{A diagram representing a HMM. \(y_1, y_2, ..., y_\tau\) are observations; they are emitted with probability \(p_\mu(y_t | x_t)\). \(x_t\) is the hidden state at time \(t\), where the next state is transitioned to with probability \(p_\mu(x_{t+1} | x_t)\)}
    \label{fig:hmm}
    \vspace{2ex}
    \begin{tikzpicture}[x=1pt,y=1pt]
        \matrix [column sep=10] {
            \node [box] (e1) {$y_1$}; & \node [box] (e2) {$y_2$}; & \node [box,draw=none] (da) {$\dots$}; & \node [box] (et) {$y_t$}; & \node [box,draw=none] (db) {$\dots$}; & \node [box] (etauprev) {$y_{\tau - 1}$}; & \node [box] (etau) {$y_\tau$}; \\
        };
        \node [box,circle,below=25 of et] (st) {$x_t$};
        \draw [->,thick] (e1) -- (e2);
        \draw [->,thick] (e2) -- (da);
        \draw [->,thick] (da) -- (et);
        \draw [->,thick] (et) -- (db);
        \draw [->,thick] (db) -- (etauprev);
        \draw [->,thick] (etauprev) -- (etau);
        \draw [->,thick] (st) -- (et) node[midway,right] {$p_\mu(y_t | x_t)$};
        \draw [->,thick] (st) to [loop,looseness=6,out=315,in=225] node[midway,below] {$p_\mu(x_{t+1} | x_t)$} (st);
    \end{tikzpicture}
\end{figure}

\section{Approach}\label{sec:approach}

We propose an automatic workload generation approach that leverages generative models and their unsupervised learning capabilities. In this approach, only a small set of user actions must be collected to train the model. Once fitted, the model can extrapolate realistic synthetic workloads with high entropy.

\subsection{Fitting the Model}

Consider a single HMM \(\mu\) with states \(1, 2, ...,n\) in \(S_\mu\). To fit this HMM on a hidden process, we can maximize the likelihood \(\mathcal{L}(\mu | \seq{y_t}_t) = p_\mu(\seq{y_t}_t)\) of \(\mu\) given observations from the hidden process \(\seq{y_t}_t = \seq{y_1, y_2, ..., y_t}\). Since an observation \(y_t\) is only dependant on the current state \(x_t\), we can write this probability like so:
\begin{equation}\label{eq:likelihood}
    p_\mu(\seq{y_t}_t) = \sum_{x_t} p_\mu(x_t, \seq{y_t}_t),
\end{equation}
where it can be demonstrated by recursively applying the chain rule that for \(1
\leq t \leq T\)
\begin{equation}
    \begin{multlined}
        p_\mu(x_t, \seq{y_t}_t) = \\
        p_\mu(y_t|x_t) \sum_{x_{t-1}} p_\mu(x_t|x_{t-1}) p_\mu(x_{t-1}, \seq{y_t}_{t-1}).
    \end{multlined}
\end{equation}
and with the base case
\begin{equation}\label{eq:max_likelihood}
    p_\mu(x_0, \seq{}) = p_\mu(x_0)
\end{equation}
\(p_\mu(y_t|x_t)\) is the emission probability of \(\mu\), \(p_\mu(x_t | x_t)\) is the transition probability and \(p_\mu(x_0)\) is the prior or stationary probability of the states. Then, the fitted model \(\hat{\mu}\) is one that maximizes \cref{eq:likelihood}
\begin{equation}
    \label{eq:argmax_observation}
    \hat{\mu} = \argmax_{\mu} p_\mu(\seq{y_t}_t).
\end{equation}

This recursive equation can be solved using the forward algorithm described in \cref{alg:forward}. The forward algorithm has space complexity \(O(n)\)  and time complexity \(O(nT)\) where \(n\) is the number of possible states and \(T\) is the length of the input sequence. In the context of interleaved HMM \(\mu\), there are \(K\) individual HMMs \(\mu_k\), the state of \(\mu\) must encode the states of all \(\mu_k\). Thus, no matter the encoding, the time and space complexity of the forward algorithm will be exponential with respect to \(K\). In fact, training the interleaved HMM is an intractable problem\cite{Landwehr2008-vw}.
\begin{algorithm}[H]
    \caption{The forward algorithm.}\label{alg:forward}
    \begin{algorithmic}[1]
        \Function{Forward}{$o_{1..T}$}
        \For{$i \gets 1 .. n$}
        \State $\alpha_i \gets \pi_i$
        \EndFor
        \For{$t \gets 1 .. T$}
        \For{$i \gets 1 .. n$}
        \State $\alpha_i \gets P(o_t|s_{i,t}) \sum_{s_{i, t-1}} P(s_{i, t} | s_{i, t-1}) \alpha_i$√ç
        \EndFor
        \EndFor
        \State \Return $\sum_{i} \alpha_i$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Since the forward algorithm's gradient can be automatically computed by Jax\cite{Bradbury2018-jz}, on which Flax is based, \cref{eq:argmax_observation} can be found using gradient descent. Specifically, I used the Adam\cite{Kingma2014-jj} optimizer on a synthetic dataset described in \cref{sec:dataset}. I implemented the forward algorithm in \(\log\) space for better numerical stability and thus the optimized loss is the negative log-likelihood:
\begin{equation}
    \label{eq:neg_log_likelihood}
    \argmin_{\mu} - \log P(y_{1..T} | \mu).
\end{equation}

\section{Experimental Setup}\label{sec:experimental_setup}

We have implemented an interleaved HMM using the Flax\cite{Heek2023-nl} framework.


\subsection{Research Questions}

\begin{researchquestions}
    \item\label{rq:performance} How does our approach perform for real workloads?
    \item\label{rq:ablation} How does the approach perform under different settings?
\end{researchquestions}

\subsection{Datasets}\label{sec:dataset}

\subsubsection{Apache James}

\subsubsection{OpenMRS}

\subsubsection{Google Borg Trace Version 2}

\section{Results}\label{sec:results}

\subsection*{\cref{rq:performance}: How does our approach perform for real workloads?}

\subsection*{\cref{rq:ablation}: How does our approach perform under different settings?}

\section{Discussion}\label{sec:discussion}

\subsection{Threats to Validity}

\subsubsection{Internal Validity}

\subsubsection{External Validity}

\subsubsection{Construct Validity}

\subsection{Future Work}

\section{Conclusion}\label{sec:conclusion}

\printbibliography

\end{document}